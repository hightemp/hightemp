**Блок I: Фундаментальные основы**

*   **1. Математика для Data Science:**
    *   **1.1. Линейная алгебра:**
        *   **Векторы:** Определение, операции (сложение, вычитание, умножение на скаляр), скалярное произведение, векторное произведение (в R³), норма вектора, ортогональность, линейная зависимость/независимость.
        *   **Матрицы:** Определение, типы матриц (квадратная, диагональная, единичная, нулевая, симметричная), операции (сложение, умножение на скаляр, умножение матриц, транспонирование), определитель матрицы, обратная матрица.
        *   **Системы линейных уравнений:** Методы решения (метод Гаусса, метод Крамера), ранг матрицы, теорема Кронекера-Капелли.
        *   **Векторные пространства:** Определение, базис, размерность, подпространства.
        *   **Собственные значения и собственные векторы:** Нахождение, геометрический смысл, диагонализация матриц.
        *   **Разложения матриц:** SVD (Сингулярное разложение), QR-разложение, LU-разложение (понимание их применения в DS, например, SVD в PCA и рекомендательных системах).
    *   **1.2. Математический анализ (Calculus):**
        *   **Функции одной переменной:** Пределы, непрерывность, производная (определение, правила дифференцирования, геометрический и физический смысл), исследование функций с помощью производных (монотонность, экстремумы), интеграл (неопределенный, определенный, формула Ньютона-Лейбница).
        *   **Функции нескольких переменных:** Частные производные, градиент (его связь с направлением наискорейшего роста), матрица Гессе, полный дифференциал.
        *   **Оптимизация:** Поиск локальных и глобальных экстремумов функций, условная оптимизация (метод множителей Лагранжа).
        *   **Градиентный спуск:** Алгоритм, вариации (стохастический, пакетный, мини-пакетный), скорость обучения, проблемы (локальные минимумы, плато).
    *   **1.3. Теория вероятностей:**
        *   **Основные понятия:** События, пространство элементарных исходов, аксиомы Колмогорова, классическое определение вероятности, условная вероятность, независимость событий.
        *   **Формулы:** Формула полной вероятности, формула Байеса (фундаментальна для многих ML алгоритмов).
        *   **Случайные величины:** Дискретные и непрерывные СВ, функция распределения, плотность вероятности.
        *   **Числовые характеристики СВ:** Математическое ожидание, дисперсия, стандартное отклонение, мода, медиана, квантили, ковариация, корреляция (Пирсона, Спирмена).
        *   **Основные распределения:**
            *   Дискретные: Бернулли, Биномиальное, Пуассона, Геометрическое, Гипергеометрическое.
            *   Непрерывные: Равномерное, Нормальное (Гауссово), Экспоненциальное, Гамма, Бета.
        *   **Закон больших чисел (ЗБЧ):** Различные формы (Чебышева, Бернулли, Колмогорова).
        *   **Центральная предельная теорема (ЦПТ):** Формулировка, значение для статистики.
    *   **1.4. Математическая статистика:**
        *   **Описательная статистика:** Меры центральной тенденции (среднее, медиана, мода), меры разброса (дисперсия, стд. отклонение, размах, интерквартильный размах), визуализация данных (гистограммы, ящики с усами, диаграммы рассеяния).
        *   **Теория оценок:** Точечные оценки (несмещенность, состоятельность, эффективность), метод максимального правдоподобия, метод моментов.
        *   **Интервальные оценки:** Доверительные интервалы для среднего, доли, разности средних/долей.
        *   **Проверка статистических гипотез:**
            *   Основные понятия: Нулевая и альтернативная гипотезы, уровень значимости (alpha), p-value, статистическая мощность (beta), ошибки I и II рода.
            *   Параметрические тесты: t-тесты (одновыборочный, двухвыборочный независимый/зависимый), Z-тест, F-тест (дисперсионный анализ - ANOVA), критерий хи-квадрат (для таблиц сопряженности, проверки согласия).
            *   Непараметрические тесты: Критерий Манна-Уитни, критерий Уилкоксона, критерий Краскела-Уоллиса, критерий знаков.
        *   **Корреляционный и регрессионный анализ (статистический взгляд):** Построение моделей, оценка значимости коэффициентов, коэффициент детерминации R².
    *   **1.5. Дискретная математика (Основы):**
        *   **Теория множеств:** Операции над множествами, диаграммы Венна.
        *   **Комбинаторика:** Правила суммы и произведения, перестановки, размещения, сочетания.
        *   **Теория графов:** Основные определения (вершины, ребра, степень), типы графов, обходы графов (в ширину, в глубину), кратчайшие пути (алгоритм Дейкстры). (Полезно для сетевого анализа, рекомендательных систем).

**Блок II: Инструменты и Программирование**

*   **2. Основы Программирования (Python как основной язык):**
    *   **2.1. Базовый синтаксис Python:** Переменные, типы данных (int, float, bool, str, None), операторы, условные конструкции (if/elif/else), циклы (for, while), функции, области видимости.
    *   **2.2. Структуры данных Python:** Списки (lists), кортежи (tuples), словари (dictionaries), множества (sets), их методы и особенности использования. List comprehensions, dictionary comprehensions.
    *   **2.3. Объектно-ориентированное программирование (ООП) в Python:** Классы, объекты, атрибуты, методы, наследование, полиморфизм, инкапсуляция, магические методы (__init__, __str__, etc.).
    *   **2.4. Работа с файлами:** Чтение, запись текстовых и бинарных файлов.
    *   **2.5. Модули и пакеты:** Импорт, создание собственных модулей, работа с pip и менеджерами пакетов (conda).
    *   **2.6. Виртуальные окружения:** Venv, Conda environments – зачем нужны и как использовать.
    *   **2.7. Основы отладки и обработки исключений:** Try/except/finally, типы ошибок, использование дебаггера (pdb или в IDE).
*   **3. Ключевые библиотеки Python для Data Science:**
    *   **3.1. NumPy:**
        *   Массивы ndarray: Создание, индексация, срезы, изменение формы (reshape).
        *   Векторизованные операции: Математические, логические операции над массивами.
        *   Broadcasting: Правила "растягивания" массивов для операций.
        *   Линейная алгебра в NumPy (linalg): Решение СЛУ, нахождение собственных значений/векторов, SVD.
        *   Генерация случайных чисел.
    *   **3.2. Pandas:**
        *   Структуры данных: Series, DataFrame – создание, основные атрибуты и методы.
        *   Чтение и запись данных: CSV, Excel, JSON, SQL, HTML и др. форматы.
        *   Индексация и выборка данных: .loc, .iloc, boolean indexing.
        *   Обработка пропущенных значений (NaN): isnull(), dropna(), fillna().
        *   Операции с данными: Сортировка, изменение типов данных, применение функций (apply, map, applymap).
        *   Группировка данных: GroupBy – агрегация (sum, mean, count, etc.), трансформация, фильтрация.
        *   Слияние и объединение таблиц: merge, join, concat.
        *   Работа с временными рядами: Типы данных datetime, Resampling, Rolling windows.
        *   Сводные таблицы (Pivot tables).
    *   **3.3. Визуализация данных:**
        *   **Matplotlib:** Основы (figure, axes), типы графиков (line, scatter, bar, hist, pie), настройка внешнего вида (заголовки, метки, легенды, цвета, стили). Subplots.
        *   **Seaborn:** Высокоуровневая библиотека над Matplotlib, улучшенные стили по умолчанию, статистические графики (distplot, boxplot, violinplot, pairplot, heatmap, clustermap).
        *   **Plotly:** Интерактивные графики, использование онлайн и оффлайн, создание дашбордов (Plotly Dash).
        *   **(Опционально) Bokeh, Altair.**
*   **4. Работа с данными и базами данных:**
    *   **4.1. SQL (Structured Query Language):**
        *   Основные команды: SELECT, FROM, WHERE, ORDER BY, GROUP BY, HAVING.
        *   Агрегатные функции: COUNT, SUM, AVG, MIN, MAX.
        *   Объединение таблиц: INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL OUTER JOIN, CROSS JOIN.
        *   Подзапросы (Subqueries).
        *   Оконные функции (Window Functions): ROW_NUMBER(), RANK(), DENSE_RANK(), LAG(), LEAD(), SUM()/AVG() OVER (...).
        *   Common Table Expressions (CTE).
        *   Основы проектирования баз данных: Нормальные формы (1NF, 2NF, 3NF).
        *   Работа с конкретными СУБД (например, PostgreSQL, MySQL): Типы данных, индексы.
    *   **4.2. NoSQL Базы данных (Обзорно):**
        *   Типы: Документо-ориентированные (MongoDB), Ключ-значение (Redis), Колоночные (Cassandra), Графовые (Neo4j).
        *   Сценарии использования каждого типа.
        *   Основные принципы работы (отсутствие строгой схемы, масштабируемость).
*   **5. Инструменты разработки и коллаборации:**
    *   **5.1. Git и GitHub/GitLab/Bitbucket:**
        *   Системы контроля версий: Зачем нужны?
        *   Основные команды Git: init, clone, add, commit, status, log, branch, checkout, merge, pull, push.
        *   Работа с ветками: Создание, слияние (merge), разрешение конфликтов.
        *   Pull Requests / Merge Requests: Процесс ревью кода.
        *   Файл .gitignore.
    *   **5.2. Среды разработки:**
        *   Jupyter Notebook / JupyterLab: Интерактивный анализ, прототипирование.
        *   IDE: VS Code, PyCharm (преимущества для больших проектов, отладка, рефакторинг).
    *   **5.3. Основы командной строки (Shell/Bash):** Навигация, работа с файлами и директориями (cd, ls, pwd, mkdir, rm, cp, mv), конвейеры (|), перенаправление ввода/вывода (>, >>), grep, find, basic scripting.
    *   **5.4. Docker (Основы):**
        *   Контейнеризация: Что это и зачем? (Изоляция, воспроизводимость).
        *   Основные концепции: Image, Container, Dockerfile, Docker Hub.
        *   Базовые команды: build, run, ps, stop, rm, rmi, logs.

**Блок III: Машинное обучение (Machine Learning)**

*   **6. Введение в Машинное обучение:**
    *   **6.1. Основные концепции:** Что такое ML? Задачи ML (классификация, регрессия, кластеризация, понижение размерности, генерация). Обучение с учителем, без учителя, с подкреплением.
    *   **6.2. Жизненный цикл проекта ML:** Постановка задачи -> Сбор данных -> Очистка и подготовка данных -> Разведочный анализ данных (EDA) -> Выбор и инженерия признаков -> Выбор модели -> Обучение модели -> Оценка модели -> Тюнинг гиперпараметров -> Внедрение (Deployment) -> Мониторинг.
    *   **6.3. Переобучение (Overfitting) и Недообучение (Underfitting):** Диагностика, способы борьбы (регуляризация, кросс-валидация, сбор большего количества данных, упрощение модели).
    *   **6.4. Смещение (Bias) и Разброс (Variance):** Дилемма Bias-Variance.
    *   **6.5. Кросс-валидация:** K-Fold, Stratified K-Fold, Leave-One-Out (LOO), Time Series Split. Цель и реализация.
    *   **6.6. Метрики качества моделей:**
        *   **Регрессия:** MAE, MSE, RMSE, R², Adjusted R², MAPE.
        *   **Классификация:** Accuracy, Precision, Recall (Sensitivity), F1-score, Specificity, ROC-кривая, AUC-ROC, Precision-Recall кривая, AUC-PR, LogLoss, Confusion Matrix.
        *   **Кластеризация:** Индекс Силуэта, Индекс Дэвиса-Болдина, Adjusted Rand Index (ARI), Mutual Information (MI).
*   **7. Подготовка данных и Инженерия признаков (Feature Engineering):**
    *   **7.1. Очистка данных:** Работа с пропусками (удаление, импутация - средним, медианой, модой, регрессионная импутация, KNNImputer), обнаружение и обработка выбросов (статистические методы, Isolation Forest).
    *   **7.2. Преобразование признаков:**
        *   Масштабирование/Нормализация: StandardScaler, MinMaxScaler, RobustScaler, Normalizer. Зачем и когда применять.
        *   Логарифмирование, степенные преобразования (Box-Cox, Yeo-Johnson).
        *   Дискретизация (Binning) непрерывных признаков.
    *   **7.3. Кодирование категориальных признаков:**
        *   Nominal: One-Hot Encoding, Dummy Encoding, Hashing Trick.
        *   Ordinal: Label Encoding, Ordinal Encoding.
        *   Target Encoding (Mean Encoding), CatBoost Encoding. Плюсы и минусы, проблема data leakage.
    *   **7.4. Создание новых признаков:**
        *   Полиномиальные признаки.
        *   Взаимодействия признаков (Feature Interactions).
        *   Признаки на основе дат и времени (день недели, месяц, час, праздник и т.д.).
        *   Агрегированные признаки (на основе группировок).
        *   Доменные признаки (основанные на знании предметной области).
    *   **7.5. Отбор признаков (Feature Selection):**
        *   Методы фильтрации (Filter Methods): На основе корреляции, ANOVA F-value, Хи-квадрат, Mutual Information.
        *   Методы обертывания (Wrapper Methods): Recursive Feature Elimination (RFE), Forward/Backward Selection.
        *   Встроенные методы (Embedded Methods): LASSO регуляризация, важность признаков из древовидных моделей (Tree-based Feature Importance).
        *   Использование PCA для отбора (с осторожностью).
*   **8. Обучение с учителем (Supervised Learning):**
    *   **8.1. Линейные модели:**
        *   **Линейная регрессия:** Метод наименьших квадратов (МНК), предположения модели, оценка качества, интерпретация коэффициентов.
        *   **Регуляризация:** L1 (Lasso), L2 (Ridge), ElasticNet. Геометрическая интерпретация, влияние на коэффициенты, использование для отбора признаков (Lasso).
        *   **Логистическая регрессия:** Для задач классификации, сигмоидная функция, функция потерь (LogLoss), интерпретация коэффициентов (отношение шансов), проблема разделяемости.
        *   **Support Vector Machines (SVM):**
            *   Линейный SVM: Идея максимального зазора, опорные векторы, Soft Margin SVM (параметр C).
            *   Нелинейный SVM: Kernel Trick (Ядерный трюк) - полиномиальное, RBF (Гауссово), сигмоидное ядра. Параметры (C, gamma).
            *   Support Vector Regression (SVR).
    *   **8.2. Древовидные модели:**
        *   **Дерево решений (Decision Tree):** Алгоритмы построения (ID3, C4.5, CART), критерии ветвления (Gini impurity, Entropy, Variance Reduction), проблема переобучения, стрижка дерева (pruning). Визуализация и интерпретация.
        *   **Ансамблевые методы:**
            *   **Бэггинг (Bagging):** Идея, Random Forest (случайный лес) – построение, параметры (n_estimators, max_features, max_depth), OOB (Out-of-Bag) оценка, важность признаков.
            *   **Бустинг (Boosting):**
                *   AdaBoost (Adaptive Boosting): Принцип последовательного обучения, взвешивание объектов и моделей.
                *   Gradient Boosting Machines (GBM): Обучение на ошибках предыдущих моделей, градиентный спуск в пространстве функций.
                *   **XGBoost:** Оптимизации GBM (регуляризация, параллелизация, обработка пропусков, DART). Ключевые гиперпараметры.
                *   **LightGBM:** Дальнейшие оптимизации (GOSS, EFB), рост дерева по листьям (leaf-wise). Ключевые гиперпараметры.
                *   **CatBoost:** Работа с категориальными признаками (ordered boosting, target-based statistics), симметричные деревья. Ключевые гиперпараметры.
            *   **Стекинг (Stacking) / Блендинг (Blending):** Комбинирование предсказаний разных моделей с помощью мета-модели.
    *   **8.3. Метрические алгоритмы:**
        *   **K-Nearest Neighbors (KNN):** Алгоритм для классификации и регрессии, выбор метрики расстояния (Евклидово, Манхэттенское, Минковского), выбор числа соседей (k), взвешивание соседей, проклятие размерности.
    *   **8.4. Наивные Байесовские классификаторы (Naive Bayes):**
        *   Основан на теореме Байеса и предположении о независимости признаков.
        *   Варианты: GaussianNB, MultinomialNB, BernoulliNB. Применение (особенно в NLP).
*   **9. Обучение без учителя (Unsupervised Learning):**
    *   **9.1. Кластеризация:**
        *   **K-Means:** Алгоритм, выбор числа кластеров (k) – метод локтя, силуэт; проблема инициализации центроидов (k-means++), чувствительность к масштабу и форме кластеров.
        *   **Иерархическая кластеризация:** Агломеративная (снизу вверх) и дивизивная (сверху вниз), дендрограммы, методы связи (ward, complete, average).
        *   **DBSCAN:** Плотность-ориентированный метод, понятия core point, border point, noise; параметры (eps, min_samples), не требует задания числа кластеров, находит кластеры произвольной формы.
        *   **Gaussian Mixture Models (GMM):** Вероятностный подход, EM-алгоритм, мягкая кластеризация.
    *   **9.2. Понижение размерности:**
        *   **Principal Component Analysis (PCA):** Метод главных компонент. Нахождение главных компонент (собственные векторы ковариационной матрицы), объясненная дисперсия, выбор числа компонент, использование для визуализации и сжатия данных. Предположения и ограничения.
        *   **Linear Discriminant Analysis (LDA):** Метод с учителем, ищет проекцию, максимизирующую разделение между классами.
        *   **t-Distributed Stochastic Neighbor Embedding (t-SNE):** Нелинейный метод, в основном для визуализации высокоразмерных данных в 2D/3D. Параметр perplexity. Не сохраняет глобальную структуру.
        *   **UMAP (Uniform Manifold Approximation and Projection):** Альтернатива t-SNE, часто лучше сохраняет глобальную структуру, быстрее.
        *   **Автоэнкодеры (Autoencoders):** Нейросетевой подход (см. Блок V).
    *   **9.3. Поиск аномалий (Anomaly Detection / Outlier Detection):**
        *   Статистические методы (Z-score, IQR).
        *   Isolation Forest.
        *   One-Class SVM.
        *   Local Outlier Factor (LOF).
*   **10. Оценка и Выбор Моделей:**
    *   **10.1. Сравнение моделей:** Использование кросс-валидации и статистических тестов для сравнения производительности моделей.
    *   **10.2. Настройка гиперпараметров:**
        *   Grid Search CV.
        *   Randomized Search CV.
        *   Байесовская оптимизация (Hyperopt, Optuna, Scikit-optimize).
    *   **10.3. Интерпретация моделей (Model Interpretability):**
        *   Важность признаков (Feature Importance): Permutation Importance, встроенная важность (для деревьев, линейных моделей).
        *   Локальная интерпретация: LIME (Local Interpretable Model-agnostic Explanations), SHAP (SHapley Additive exPlanations).
        *   Частичные зависимости (Partial Dependence Plots - PDP), Individual Conditional Expectation (ICE) plots.

**Блок IV: Специализированные области ML**

*   **11. Работа с Временными Рядами (Time Series Analysis):**
    *   **11.1. Основные понятия:** Стационарность (строгая, слабая), автокорреляция (ACF), частичная автокорреляция (PACF), тренд, сезонность, цикличность, шум. Тесты на стационарность (Дики-Фуллера, KPSS).
    *   **11.2. Преобразования рядов:** Дифференцирование, логарифмирование, удаление тренда и сезонности.
    *   **11.3. Классические модели:**
        *   Скользящие средние (Moving Averages - MA).
        *   Экспоненциальное сглаживание: Простое (SES), Хольта (линейный тренд), Хольта-Винтерса (тренд + сезонность).
        *   ARIMA (Авторегрессия - Интегрированное скользящее среднее): Модели AR(p), MA(q), ARMA(p,q), ARIMA(p,d,q). Порядок модели (p, d, q).
        *   SARIMA (Сезонная ARIMA).
        *   ARCH/GARCH модели (для моделирования волатильности).
    *   **11.4. Машинное обучение для временных рядов:**
        *   Создание признаков из временных рядов (лаговые переменные, скользящие статистики).
        *   Использование стандартных ML моделей (линейная регрессия, деревья) на созданных признаках.
        *   Facebook Prophet: Аддитивная модель, хорошо работает с сезонностью и праздниками.
    *   **11.5. Глубокое обучение для временных рядов:** RNN, LSTM, GRU (см. Блок V).
    *   **11.6. Оценка моделей временных рядов:** Walk-forward validation (скользящий контроль), метрики (MAE, RMSE, MAPE).
*   **12. Обработка Естественного Языка (Natural Language Processing - NLP):**
    *   **12.1. Основы и предобработка текста:**
        *   Токенизация (по словам, по предложениям).
        *   Нормализация: Приведение к нижнему регистру, удаление пунктуации, цифр.
        *   Стемминг (Porter, Snowball) и Лемматизация (WordNet, pymorphy2 для русского).
        *   Удаление стоп-слов.
        *   Работа с регулярными выражениями (regex) для очистки текста.
    *   **12.2. Векторизация текста (Text Representation):**
        *   Bag-of-Words (BoW): CountVectorizer, проблемы.
        *   TF-IDF (Term Frequency-Inverse Document Frequency): TfidfVectorizer, взвешивание слов.
        *   N-граммы (символьные, словесные).
        *   Word Embeddings (Векторные представления слов):
            *   Статические: Word2Vec (CBOW, Skip-gram), GloVe, FastText. Идея, обучение, свойства (семантическая близость, аналогии).
            *   Контекстуализированные: ELMo, BERT, GPT (см. Блок V).
    *   **12.3. Задачи NLP:**
        *   Классификация текстов (спам-фильтры, анализ тональности/sentiment analysis, определение тематики).
        *   Извлечение информации (Information Extraction): Named Entity Recognition (NER), Relation Extraction.
        *   Машинный перевод (Machine Translation).
        *   Генерация текста (Text Generation).
        *   Суммаризация текста (Text Summarization).
        *   Вопросно-ответные системы (Question Answering).
        *   Тематическое моделирование (Topic Modeling): Latent Dirichlet Allocation (LDA), NMF.
    *   **12.4. Библиотеки для NLP:** NLTK, spaCy, scikit-learn (для векторизации и моделей), Gensim (для Word2Vec, LDA), Transformers (Hugging Face).
*   **13. Рекомендательные системы:**
    *   **13.1. Основные подходы:**
        *   Content-Based Filtering: Рекомендации на основе схожести контента/атрибутов товаров/пользователей.
        *   Collaborative Filtering (CF):
            *   User-based CF: Найти похожих пользователей, рекомендовать то, что им нравится.
            *   Item-based CF: Найти похожие товары, рекомендовать товары, похожие на те, что понравились пользователю.
            *   Проблема холодного старта (Cold Start Problem).
        *   Matrix Factorization (Матричные разложения): SVD, ALS. Представление пользователей и товаров в латентном пространстве.
    *   **13.2. Гибридные методы:** Комбинирование различных подходов.
    *   **13.3. Оценка качества рекомендательных систем:** Метрики ранжирования (Precision@k, Recall@k, MAP, NDCG), бизнес-метрики (конверсия, CTR). A/B тестирование.
*   **14. Компьютерное зрение (Computer Vision - CV) - Основы:**
    *   **14.1. Работа с изображениями:** Форматы, представление (пиксели, каналы), базовые операции (изменение размера, обрезка, аугментация). Библиотеки: OpenCV, Pillow.
    *   **14.2. Классические методы:** Извлечение признаков (SIFT, SURF, HOG), простые классификаторы.
    *   **14.3. Глубокое обучение для CV:** CNN (см. Блок V).
    *   **14.4. Задачи CV:** Классификация изображений, детекция объектов, сегментация изображений.

**Блок V: Глубокое обучение (Deep Learning)**

*   **15. Основы Нейронных Сетей:**
    *   **15.1. Биологическая инспирация:** Нейроны, синапсы.
    *   **15.2. Перцептрон:** Линейный классификатор, функция активации (пороговая).
    *   **15.3. Многослойный перцептрон (MLP):** Прямое распространение (Forward Propagation), функция потерь (Loss Function), обратное распространение ошибки (Backpropagation), градиентный спуск и его вариации (SGD, Momentum, RMSProp, Adam).
    *   **15.4. Функции активации:** Sigmoid, Tanh, ReLU и ее вариации (Leaky ReLU, ELU, Swish). Проблема затухающих/взрывающихся градиентов.
    *   **15.5. Регуляризация в нейронных сетях:** L1/L2 регуляризация весов, Dropout, Batch Normalization, Early Stopping.
    *   **15.6. Инициализация весов:** Xavier/Glorot, He.
    *   **15.7. Фреймворки глубокого обучения:**
        *   **TensorFlow:** Основные концепции (тензоры, графы вычислений), Keras API (Sequential, Functional), tf.data для построения пайплайнов данных.
        *   **PyTorch:** Основные концепции (тензоры, динамические графы), модули (nn.Module), оптимизаторы, DataLoader.
*   **16. Сверточные Нейронные Сети (Convolutional Neural Networks - CNN):**
    *   **16.1. Архитектура:** Сверточные слои (Convolutional layers) - фильтры (ядра), карты признаков, stride, padding. Слои пулинга (Pooling layers) - Max Pooling, Average Pooling. Полносвязные слои (Fully Connected layers).
    *   **16.2. Популярные архитектуры:** LeNet, AlexNet, VGG, ResNet (Residual Networks), Inception (GoogLeNet), MobileNet, EfficientNet.
    *   **16.3. Применения:** Классификация изображений, Детекция объектов (R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD), Сегментация изображений (FCN, U-Net).
    *   **16.4. Transfer Learning (Перенос обучения):** Использование предобученных моделей, fine-tuning.
    *   **16.5. Аугментация данных для изображений.**
*   **17. Рекуррентные Нейронные Сети (Recurrent Neural Networks - RNN):**
    *   **17.1. Архитектура:** Обработка последовательностей, скрытое состояние, проблема исчезающих/взрывающихся градиентов в простых RNN.
    *   **17.2. LSTM (Long Short-Term Memory):** Ячейки памяти, гейты (входной, забывания, выходной). Решение проблемы долговременных зависимостей.
    *   **17.3. GRU (Gated Recurrent Unit):** Упрощенная версия LSTM, гейты обновления и сброса.
    *   **17.4. Применения:** Обработка текста (NLP), анализ временных рядов, распознавание речи.
    *   **17.5. Двунаправленные RNN (Bidirectional RNN).**
    *   **17.6. Модели Seq2Seq (Sequence-to-Sequence):** Архитектура Encoder-Decoder, применение в машинном переводе, генерации текста.
*   **18. Механизм Внимания (Attention) и Трансформеры:**
    *   **18.1. Механизм Внимания:** Идея взвешивания входных элементов при генерации выходных. Применение в Seq2Seq моделях.
    *   **18.2. Архитектура Трансформера ("Attention Is All You Need"):** Self-Attention, Multi-Head Attention, Positional Encoding, структура Encoder-Decoder на основе Attention.
    *   **18.3. BERT (Bidirectional Encoder Representations from Transformers):** Предобученная модель для NLP на основе энкодера Трансформера, fine-tuning под конкретные задачи (классификация, NER, QA).
    *   **18.4. GPT (Generative Pre-trained Transformer) и его варианты:** Модели на основе декодера Трансформера, фокус на генерации текста.
    *   **18.5. Vision Transformer (ViT):** Применение Трансформеров в компьютерном зрении.
*   **19. Другие архитектуры и направления DL:**
    *   **19.1. Автоэнкодеры (Autoencoders):** Нелинейное понижение размерности, шумоподавление, генерация данных. Variational Autoencoders (VAE).
    *   **19.2. Generative Adversarial Networks (GANs):** Генеративные состязательные сети. Генератор и Дискриминатор, процесс обучения. Применения (генерация изображений, style transfer).
    *   **19.3. Graph Neural Networks (GNN):** Работа с графовыми структурами данных.
    *   **19.4. Reinforcement Learning (Обучение с подкреплением) - Введение:** Агент, среда, состояния, действия, награды, политика. Markov Decision Processes (MDP). Q-learning, Deep Q-Networks (DQN). (Более специализированная область).

**Блок VI: Инженерия данных и Большие Данные (Data Engineering & Big Data)**

*   **20. Экосистемы Больших Данных:**
    *   **20.1. Концепции Big Data:** 3V (Volume, Velocity, Variety) и их расширения.
    *   **20.2. Hadoop Ecosystem (Обзорно):**
        *   HDFS (Hadoop Distributed File System): Архитектура, отказоустойчивость.
        *   MapReduce: Парадигма распределенных вычислений (концепция).
        *   YARN: Управление ресурсами кластера.
        *   Hive: SQL-подобный интерфейс для данных в Hadoop.
        *   HBase: NoSQL база данных поверх HDFS.
    *   **20.3. Apache Spark:**
        *   Архитектура Spark: Драйвер, исполнители (executors), RDD (Resilient Distributed Datasets).
        *   Spark Core API: Работа с RDD (transformations, actions), ленивые вычисления.
        *   Spark SQL: DataFrames и Datasets API, работа со структурированными данными, интеграция с Hive.
        *   Spark Streaming / Structured Streaming: Обработка потоковых данных.
        *   MLlib: Библиотека машинного обучения Spark.
        *   GraphX: Обработка графов.
        *   Оптимизация Spark приложений.
*   **21. Потоковая обработка данных (Stream Processing):**
    *   **21.1. Концепции:** События, потоки, окна (tumbling, sliding, session), водяные знаки (watermarks).
    *   **21.2. Инструменты:**
        *   Apache Kafka: Распределенная система обмена сообщениями (брокер сообщений). Продюсеры, консюмеры, топики, партиции.
        *   Spark Streaming / Structured Streaming (уже упомянуто).
        *   Apache Flink: Другой мощный фреймворк для потоковой обработки.
        *   Облачные аналоги (AWS Kinesis, Google Cloud Pub/Sub, Azure Event Hubs).
*   **22. Хранилища данных (Data Warehousing - DWH):**
    *   **22.1. Концепции:** OLTP vs OLAP, ETL (Extract, Transform, Load) vs ELT, Схемы "Звезда" и "Снежинка", Факты и Измерения (Facts & Dimensions), Медленно меняющиеся измерения (Slowly Changing Dimensions - SCD).
    *   **22.2. Инструменты и Платформы:**
        *   Традиционные DWH (Teradata, Oracle).
        *   Облачные DWH: Amazon Redshift, Google BigQuery, Snowflake, Azure Synapse Analytics.
        *   Инструменты ETL/ELT: Apache NiFi, Talend, Informatica, облачные сервисы (AWS Glue, GCP Dataflow, Azure Data Factory).
*   **23. Оркестрация пайплайнов данных:**
    *   **23.1. Зачем нужна:** Управление зависимостями задач, расписание, мониторинг, перезапуски.
    *   **23.2. Инструменты:**
        *   Apache Airflow: DAGs (Directed Acyclic Graphs), операторы, сенсоры.
        *   Prefect, Dagster (современные альтернативы).
        *   Облачные аналоги (AWS Step Functions, GCP Cloud Composer, Azure Logic Apps).

**Блок VII: MLOps (Machine Learning Operations)**

*   **24. Внедрение моделей (Model Deployment):**
    *   **24.1. Стратегии:** Batch prediction, Real-time prediction (API), Streaming prediction.
    *   **24.2. Создание API для моделей:** Flask, FastAPI.
    *   **24.3. Контейнеризация моделей:** Docker (уже упомянуто).
    *   **24.4. Сервинг моделей:** TensorFlow Serving, TorchServe, Seldon Core, KServe (KFServing), облачные платформы (AWS SageMaker Endpoints, GCP AI Platform/Vertex AI Prediction, Azure ML Endpoints).
*   **25. Мониторинг моделей:**
    *   **25.1. Типы дрифта:** Data Drift (изменение распределения входных данных), Concept Drift (изменение зависимостей между входом и выходом).
    *   **25.2. Мониторинг производительности модели:** Отслеживание метрик качества на реальных данных.
    *   **25.3. Мониторинг данных:** Отслеживание статистики и распределений входных и выходных данных.
    *   **25.4. Инструменты:** Prometheus, Grafana, специализированные библиотеки/платформы (Evidently AI, WhyLabs, Arize).
*   **26. Воспроизводимость и Управление Экспериментами:**
    *   **26.1. Отслеживание экспериментов:** Логирование параметров, метрик, артефактов (моделей, данных).
    *   **26.2. Инструменты:** MLflow, Weights & Biases (W&B), DVC (Data Version Control).
*   **27. CI/CD для Машинного Обучения:**
    *   **27.1. Концепции:** Автоматизация тестирования, сборки, развертывания и мониторинга ML-пайплайнов.
    *   **27.2. Инструменты:** Jenkins, GitLab CI/CD, GitHub Actions, Tekton, Kubeflow Pipelines. Интеграция с инструментами из п. 26.

**Блок VIII: Коммуникация, Бизнес и Этика**

*   **28. Визуализация и Коммуникация Результатов:**
    *   **28.1. Продвинутая Визуализация:** Интерактивные дашборды (Dash, Streamlit, Tableau, Power BI, Looker). Принципы эффективной визуализации (Тафти, Фью).
    *   **28.2. Data Storytelling:** Построение нарратива на основе данных, адаптация под аудиторию (техническую, бизнес).
    *   **28.3. Подготовка отчетов и презентаций.**
*   **29. Бизнес-аспект Data Science:**
    *   **29.1. Понимание бизнес-задачи:** Трансляция бизнес-проблемы в DS-задачу, определение KPI.
    *   **29.2. A/B Тестирование и Экспериментальный Дизайн:** Планирование, проведение и анализ результатов онлайн-экспериментов. Статистическая значимость, мощность.
    *   **29.3. Экономика DS-проектов:** Оценка ROI, затрат на разработку и поддержку.
    *   **29.4. Взаимодействие с заказчиками и командой.**
*   **30. Этика и Ответственность в Data Science:**
    *   **30.1. Приватность данных:** Анонимизация, псевдонимизация, GDPR, CCPA.
    *   **30.2. Предвзятость (Bias) и Справедливость (Fairness):** Источники предвзятости в данных и алгоритмах, метрики справедливости, методы смягчения предвзятости.
    *   **30.3. Интерпретируемость и Объяснимость (Interpretability & Explainability):** Почему важны, методы (LIME, SHAP уже упомянуты).
    *   **30.4. Воспроизводимость исследований.**
    *   **30.5. Принципы ответственного ИИ (Responsible AI).**
