Источник: https://habr.com/ru/companies/first/articles/683870/

# Как настроить Nginx в качестве балансировщика нагрузки

 
## Что такое балансировка нагрузки?

  
Балансировка нагрузки подразумевает эффективное распределение входящего сетевого трафика между группой [бэкенд](https://ru.wikipedia.org/wiki/%D0%A4%D1%80%D0%BE%D0%BD%D1%82%D0%B5%D0%BD%D0%B4_%D0%B8_%D0%B1%D1%8D%D0%BA%D0%B5%D0%BD%D0%B4)-серверов. Задача же регулятора — распределить нагрузку между несколькими установленными бэкенд-серверами.  
  
Существует несколько типов балансировщиков нагрузки:  
  

- Балансировщик нагрузки приложений.
- Сетевой балансировщик нагрузки.
- Балансировщик нагрузки шлюза.
- Классический балансировщик нагрузки.

  
Также существует множество непосредственно самих балансировщиков нагрузки, и все они имеют различные варианты использования:  
  

1. [Haproxy](https://www.haproxy.org/)
2. [Nginx](https://www.nginx.com/)
3. [mod_athena](http://ath.sourceforge.net/mod_athena_doc/html/index.html)
4. [Varnish](http://opentsdb.net/docs/build/html/user_guide/utilities/varnish.html)
5. [Balance](https://balance.inlab.net/)
6. [Linux Virtual Server (LVS)](http://www.linuxvirtualserver.org/whatis.html)

  
В этой статье мы будем рассматривать **Nginx**.  
  

## Что такое балансировка нагрузки Nginx?

  
[Nginx](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/) — это высокопроизводительный веб-сервер, который также может использоваться в качестве регулятора нагрузки — это процесс распределения веб-трафика между несколькими серверами с помощью Nginx.  
  
Он гарантирует, что ни один сервер не будет перегружен и что все запросы будут обработаны своевременно. Nginx использует различные алгоритмы для определения оптимального распределения трафика, а также может быть настроен для обеспечения отказоустойчивости в случае выхода из строя одного из серверов.  
  
Вы можете использовать либо [**Nginx open source**](https://wtit.com/compare-nginx-models-nginx-open-source-os-vs-plus-free-vs-paid/), либо [**Nginx Plus**](https://www.nginx.com/products/nginx/) для балансировки нагрузки HTTP-трафика на группу серверов.  
  
Лично я использую Nginx open source для настройки своих стабилизаторов нагрузки, и именно его я собираюсь показать вам в этой статье.  
  

## Преимущества

  
Балансировка нагрузки помогает масштабировать приложение, справляясь со скачками трафика без увеличения расходов на облако. Она также помогает устранить проблему единой точки отказа. Поскольку нагрузка является распределённой, то в случае сбоя одного из серверов — сервис всё равно продолжит работу.  
  

## Настраиваем Nginx

  

### Установка Nginx

  
Первым шагом является установка Nginx. Он может быть установлен в Debian, Ubuntu или CentOS. Я собираюсь использовать Ubuntu, которую уже настроил на своём виртуальном сервере.  
  

```
sudo apt-get updatesudo apt-get install nginx
```

  

### Настройка Nginx в качестве балансировщика нагрузки

  
Создадим новый файл конфигурации для балансировщика нагрузки:  
  

```
cd /etc/nginx/sites-available/sudo nano default
```

  

```
http {   upstream app{      server 10.2.0.100;      server 10.2.0.101;      server 10.2.0.102;   }   # Этот сервер принимает весь трафик на порт 80 и передает его вышестоящему потоку.   # Обратите внимание, что имя вышестоящего потока и proxy_pass должны совпадать.   server {      listen 80;            server_name mydomain.com;      location / {          include proxy_params;                    proxy_pass http://app;                    proxy_redirect off;          proxy_http_version 1.1;          proxy_set_header Upgrade $http_upgrade;          proxy_set_header Connection "upgrade";      }   }}
```

  
В файле необходимо определить директиву **upstream** и **server**. _Upstream определяет, куда Nginx будет передавать запросы после их получения_. Она содержит IP-адреса группы серверов (бэкенда), на которые могут быть отправлены запросы в зависимости от выбранного метода регулирования нагрузки. По умолчанию Nginx использует метод балансировки **round-robin** для распределения мощности между серверами.  
  
**Сегмент server** определяет порт 80, через который Nginx будет получать запросы. Он также содержит переменную **proxy_pass**.  
  
Переменная **proxy_pass** используется для указания NGINX, _куда отправлять получаемый трафик_. В данном случае переменная proxy_pass указывает на 3 сервера. Это позволяет NGINX направлять получаемый трафик на любой из IP-адресов вышестоящих серверов. Nginx одновременно выступает в роли **обратного прокси** и **балансировщика нагрузки**.  
  
**Обратный прокси** — это сервер, _который находится между бэкенд-серверами и перехватывает запросы от клиентов_.  
  

### Выбор метода балансировки нагрузки

  
Следующий шаг — определение **метода** распределения нагрузки. Существует несколько методов, которые мы можем использовать. К ним относятся:  
  

#### Round Robin

  
**Round Robin** — это метод балансировки нагрузки, при котором каждому серверу в кластере предоставляется равная возможность обрабатывать запросы. Этот метод часто используется в веб-серверах, где каждый запрос сервера равномерно распределяется между серверами.  
  
Мощность распределяется поочерёдно, что означает, что у каждого сервера будет своё время для выполнения запроса. Например, если у вас есть три [вышестоящих](https://en.wikipedia.org/wiki/Upstream_server) сервера, A, B и C, то балансировщик нагрузки сначала распределит нагрузку на A, затем на B и, наконец, на C, прежде чем перераспределить нагрузку на A. Этот метод довольно прост, но имеет некоторую долю ограничений.  
  
Одно из ограничений заключается в том, что _некоторые серверы будут простаивать_ просто потому, что они будут ждать своей очереди. В нашем примере, если A получит задание и выполнит его за секунду, это будет означать, что он будет простаивать до следующего задания. По умолчанию _для распределения нагрузки между серверами Nginx использует именно метод round robin_.  
  

#### Round Robin с добавлением веса

  
Чтобы решить проблему **простоя серверов**, мы можем использовать **server weights** (серверные веса), чтобы указать Nginx, какие серверы должны иметь наибольший приоритет. Weighted Round Robin — _один из самых популярных методов балансировки нагрузки_, используемых сегодня.  
  
Этот метод предполагает присвоение веса каждому серверу, а затем распределение трафика между серверами на основе этих весов. Это гарантирует, что серверы с большей пропускной способностью получат больше трафика, и поможет предотвратить перегрузку какого-либо из серверов.  
  
Этот метод часто используется в сочетании с другими методами, такими как [**Session Persistence**](https://www.nginx.com/resources/glossary/session-persistence/), для обеспечения равномерного распределения мощностей на все серверы. Сервер приложения _с наибольшим параметром веса будет иметь приоритет (больше трафика)_ по сравнению с сервером с наименьшим числом (весом).  
  
Для того чтобы включить весовые параметры сервера, нужно обновить конфигурацию Nginx:  
  

```
http {   upstream app{      server 10.2.0.100 weight=5;      server 10.2.0.101 weight=3;      server 10.2.0.102 weight=1;   }  server {      listen 80;      location / {          proxy_pass http://app;      }   }}
```

  

#### Least Connection

  
Метод наименьшего числа соединений (Least Connection) — это популярная техника, используемая для равномерного распределения рабочей нагрузки между несколькими серверами. Метод работает путём маршрутизации каждого нового запроса на соединение — на сервер с _наименьшим количеством активных соединений_. Это гарантирует, что все серверы используются одинаково и ни один из них не перегружен.  
  

```
http {   upstream app{      least_conn;      server 10.2.0.100;      server 10.2.0.101;      server 10.2.0.102;   }   server {      listen 80;      location / {          proxy_pass http://app;      }   }}
```

  

#### Least Connection с добавлением веса

  
Данный метод используется для распределения рабочей нагрузки между несколькими вычислительными ресурсами (такими как серверы) с целью _оптимизации производительности и минимизации времени отклика_. Этот метод учитывает количество активных соединений на каждом сервере и присваивает соответствующие веса. Целью является распределение рабочей нагрузки таким образом, чтобы сбалансировать нагрузку и минимизировать время отклика.  
  

```
http {   upstream app{      least_conn;      server 10.2.0.100 weight=5;      server 10.2.0.101 weight=4;      server 10.2.0.102 weight=1;   }   server {      listen 80;      location / {          proxy_pass http://app;      }   }}
```

  

#### IP Hash

  
Метод балансировки IP Hash использует _алгоритм хэширования для определения того, какой сервер должен получить каждый из входящих пакетов_. Это полезно, когда за одним IP-адресом находится несколько серверов, и вы хотите убедиться, что каждый пакет с IP-адреса клиента направляется на один и тот же сервер. Он берёт _IP-адрес источника и IP-адрес назначения_ и _создаёт уникальный хэш-ключ_. Затем он используется для распределения клиента между определёнными серверами.  
  
Это очень важный момент в случае [развёртывания canary](https://semaphoreci.com/blog/what-is-canary-deployment). Это позволяет нам, разработчикам, выпускать изменения для отдельной части пользователей, чтобы они могли всё протестировать и предоставить отзывы, прежде чем отправлять их в релиз.  
  
Преимущество этого подхода в том, что он может обеспечить более высокую производительность, чем другие методы, такие как round-robin.  
  

```
http {   upstream app{      ip_hash;      server 10.2.0.100;      server 10.2.0.101;      server 10.2.0.102;   }   server {      listen 80;      location / {          proxy_pass http://app;      }   }}
```

  

#### URL Hash

  
Регулировка нагрузки URL Hash также использует алгоритм хэширования для определения того, какой сервер получит каждый из запросов на основе URL.  
  
Он также похож на метод балансировки IP Hash, но разница в том, что мы хэшируем конкретные URL, а не IP. Это гарантирует, что все запросы равномерно распределяются между серверами, обеспечивая лучшую производительность и надёжность.  
  

### Перезапустите Nginx

  
После того как вы настроили регулировщик нагрузки с нужным вам методом балансировки, вы можете перезапустить Nginx, чтобы изменения вступили в силу.  
  

```
sudo systemctl restart nginx
```

  

## Проксирование HTTP-трафика на группу серверов

  
Прокси-сервер — это сервер, который _выступает в качестве посредника между клиентом и другим сервером_. Прокси-сервер может использоваться для предоставления клиентам доступа к группе серверов, например, группе веб-серверов, путём перенаправления запросов от клиента на соответствующий сервер.  
  
Прокси-сервер также может обеспечивать [**кэширование**](https://www.iankumu.com/blog/laravel-cache/), что позволяет повысить производительность за счёт снижения необходимости посылать запросы на сервер для каждого просмотра страницы. Nginx может действовать как _прямой, так и обратный прокси-сервер_. **Разница** между прямым и обратным прокси заключается в том, что _прямой прокси находится перед несколькими клиентами_, а _обратный прокси — перед несколькими бэкендами_.  
  
Прямой прокси _защищает идентификационные данные клиентов_, а обратный прокси _защищает данные внутренних серверов_.  
  

![](https://habrastorage.org/r/w1560/webt/mz/a8/hd/mza8hd2xclxvqin9_psjmsbwkxm.jpeg)

_Прямой прокси_  
  

![](https://habrastorage.org/r/w1560/webt/et/hk/zo/ethkzokqburp8__4chj5hibw8ky.jpeg)

_Обратный прокси_  
  

## Балансировка нагрузки с включённым HTTPS

  
Распределение нагрузки с включённым HTTPS — это отличный способ повысить производительность вашего сайта. Используя балансировщик, вы можете распределить рабочую нагрузку вашего сайта между несколькими серверами, что поможет снизить потребление ресурсов каждым отдельным сервером.  
  
Кроме того, включив HTTPS, вы можете обеспечить шифрование всех соединений между серверами и посетителями вашего сайта, что поможет обеспечить дополнительную безопасность для вашего сайта.  
  
Однако у него есть и ряд определённых проблем. Одна из них заключается в том, что для каждого запроса необходимо выполнить [**TLS-рукопожатие**](https://www.cloudflare.com/learning/ssl/what-happens-in-a-tls-handshake/) и [**поиск DNS**](https://www.techopedia.com/definition/29029/dns-lookup), что может привести к задержке ответов.  
  
Я предпочитаю, чтобы мои _бэкенд-серверы приложений взаимодействовали с регулятором нагрузки по протоколу HTTP_ ([SSL-терминация](https://docs.nginx.com/nginx/admin-guide/security-controls/terminating-ssl-http/)), а _балансировщик нагрузки взаимодействовал с клиентами по протоколу HTTPS_. Таким образом, при маршрутизации запросов к серверам приложений не нужно учитывать TLS-рукопожатия и поиск DNS.  
  

## Проверка работоспособности

  
Балансировщики нагрузки часто используются для распределения входящего трафика между группой серверов, чтобы избежать перегрузки какого-либо одного сервера. Проверка работоспособности является важной частью обеспечения правильной маршрутизации трафика балансировщиком нагрузки, поскольку она помогает определить, когда сервер не работает или работает неправильно.  
  
Nginx может отслеживать здоровье наших HTTP-серверов в вышестоящей группе. Также он может выполнять _пассивные проверки состояния_ и _активные проверки состояния_.  
  

### Пассивные проверки работоспособности

  
Nginx анализирует операции по мере их выполнения и пытается восстановить неудачные соединения для пассивной проверки работоспособности. Он идентифицирует сервер как недоступный и временно прекращает передачу запросов к нему, пока тот снова не будет признан активным, если операцию всё ещё невозможно возобновить.  
  
Чтобы пометить вышестоящий сервер как недоступный, нам нужно определить два параметра в директиве upstream: **failed_timeout** и **max_fails**.  
  
**failed_timeout** задаёт время, в течение которого должно произойти определённое количество неудачных попыток, чтобы сервер был помечен как недоступный.  
  
**max_fails** задаёт количество неудачных попыток.  
  

```
upstream app{      server 10.2.0.100 max_fails=3 fail_timeout=60s;      server 10.2.0.101;      server 10.2.0.102;   }
```

  

### Активные проверки здоровья

  
Nginx может периодически проверять здоровье вышестоящих серверов, отправляя специальные запросы на проверку здоровья каждому серверу и проверяя правильность ответа.  
  

```
server {    location / {        proxy_pass http://app;        health_check;    }}
```

  
К сожалению, я не использовал активные проверки здоровья, поскольку они, похоже, применимы только для Nginx Plus. Подробнее об активных проверках здоровья вы можете прочитать [здесь](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-health-check/).  
  

## Самоуправляемый балансировщик нагрузки в сравнении с управляемым сервисом

  
Существует два основных типа балансировщиков нагрузки: **самоуправляемые** и **управляемые**. Самоуправляемые балансировщики нагрузки обычно дешевле, но для их настройки и обслуживания требуется больше технических знаний. Управляемые балансировщики нагрузки стоят дороже, но часто имеют больше функций и поддержку.  
  
Если вы хотите спокойно спать по ночам 😂, то управляемые балансировщики нагрузки гораздо лучше, так как они избавляют от головной боли, связанной с постоянной отладкой.  
  

## Ограничение количества соединений

  
Одним из способов повышения производительности балансировщика нагрузки является ограничение количества соединений, которые он пытается установить с каждым сервером. Это поможет предотвратить перегрузку балансировщиком одного из серверов и обеспечит равномерное распределение трафика между всеми ними.  
  
**Схожи ли обратный прокси и балансировщик нагрузки?**  
Да, обратный прокси и регулятор нагрузки во многом похожи. Они оба могут использоваться для повышения производительности и доступности веб-сайта или приложения, а также для распределения трафика между несколькими серверами.  
  

## Заключение

  
Балансировка нагрузки — это хороший способ распределения запросов между [инстансами](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%81%D1%82%D0%B0%D0%BD%D1%81) приложений. Он обеспечивает высокую доступность и гарантирует постоянную работоспособность сервера. Использование балансировщика Nginx для распределения нагрузки выгодно ещё и потому, что он служит как обратным прокси, так и балансировщиком нагрузки. Он также имеет открытый исходный код, поэтому вы всегда сможете получить от Nginx именно то, что нужно вам. Надеюсь, эта статья помогла вам настроить балансировщик нагрузки Nginx.  
  
Спасибо за чтение!